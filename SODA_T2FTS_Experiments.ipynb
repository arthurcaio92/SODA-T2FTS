{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SODA-T2FTS/Experiments.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqEfHEnq5fF0"
      },
      "source": [
        "**Self-Organised Direction Aware Data Partitioning Algorithm Combined With Type-2 Fuzzy Time Series**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LA2lXz25zRE"
      },
      "source": [
        "Arthur C. V. e Pinto, Thiago E. Fernandes, Petrônio C. L. Silva, Frederico G. Guimarães, Christian Wagner, Eduardo P. de Aguiar, Under publishing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIk8X83i4HUc"
      },
      "source": [
        "#SODA-T2FTS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade numpy==1.18.5"
      ],
      "metadata": {
        "id": "RILzrTu72NpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q61Hq1Sm4OI3"
      },
      "source": [
        "!git clone https://github.com/arthurcaio92/SODA_T2FTS.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLyMFh5g4WMB"
      },
      "source": [
        "pip install XlsxWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtQOSgOD63Tk"
      },
      "source": [
        "##Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAWR3TL761Xi"
      },
      "source": [
        "**CAUTION:** This task is computationally expensive and takes several hours to be performed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDq6wivX4bXV"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from SODA_T2FTS.SODA_T2FTS.sliding_window import run_sliding_window\n",
        "from SODA_T2FTS.SODA_T2FTS.datasets import get_TAIEX,get_NASDAQ,get_SP500\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "'------------------------------------------------ Data set import -------------------------------------------------'\n",
        "\n",
        "taiex_df = get_TAIEX()\n",
        "taiex = taiex_df.avg               \n",
        "taiex = taiex.to_numpy()\n",
        "\n",
        "nasdaq_df = get_NASDAQ()\n",
        "nasdaq = nasdaq_df.avg               \n",
        "nasdaq = nasdaq.to_numpy()    \n",
        "\n",
        "sp500_df = get_SP500()\n",
        "sp500 = sp500_df.Avg               \n",
        "sp500 = sp500[11500:16000]\n",
        "sp500 = sp500.to_numpy()    \n",
        "\n",
        "'------------------------------------------------ Gridsearch Parameters -------------------------------------------------'\n",
        "\n",
        "datasets = [taiex,nasdaq,sp500]\n",
        "dataset_names = ['TAIEX','NASDAQ','SP500']\n",
        "diff = 1                                      \n",
        "partition_parameters = np.arange(1,11)               \n",
        "orders = [1,2,3]\n",
        "partitioners = ['SODA']            \n",
        "mfs = ['triangular']                         \n",
        "\n",
        "\n",
        "'------------------------------------------------ Running the model -------------------------------------------------'\n",
        "\n",
        "'Builds and runs the model'\n",
        "run_sliding_window(datasets,dataset_names,diff,partition_parameters,orders,partitioners,mfs)\n",
        "\n",
        "'When done, excel files (.xlsx) will be generated for each data set with error metrics'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKsO4tNB31xO"
      },
      "source": [
        "# FTS models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0E4rFMs38QG"
      },
      "source": [
        "##Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCdmGWjqxS9s"
      },
      "source": [
        "!pip3 install -U git+git://github.com/PYFTS/pyFTS.git@5563af3079a91aac1717557224f8629370ea9f22\n",
        "\n",
        "!pip3 install -U git+https://github.com/nicolaskruchten/jupyter_pivottablejs.git\n",
        "!git clone https://github.com/petroniocandido/stac\n",
        "!pip3 install dill\n",
        "!wget https://github.com/petroniocandido/PWFTS/raw/master/benchmarks.db.gz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf5TLQCSyWiI"
      },
      "source": [
        "pip install XlsxWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Chz4YXM6yWwP"
      },
      "source": [
        "!gunzip benchmarks.db.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epYdSRqf3tZ-"
      },
      "source": [
        "##Data set import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNO_ZDhtyZqo"
      },
      "source": [
        "from pyFTS.common import Util as cUtil\n",
        "from pyFTS.benchmarks import benchmarks as bchmk, Util as bUtil\n",
        "from pyFTS.models import pwfts\n",
        "from pyFTS.common import Transformations\n",
        "tdiff = Transformations.Differential(1)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "from pyFTS.data import TAIEX, NASDAQ, SP500\n",
        "\n",
        "dataset_names = [\"TAIEX\", \"SP500\",\"NASDAQ\"]\n",
        "\n",
        "def get_dataset(name):\n",
        "    if dataset_name == \"TAIEX\":\n",
        "        return TAIEX.get_data()\n",
        "    elif dataset_name == \"SP500\":\n",
        "        return SP500.get_data()[11500:16000]\n",
        "    elif dataset_name == \"NASDAQ\":\n",
        "        return NASDAQ.get_data()\n",
        "\n",
        "train_split = 2000\n",
        "test_length = 200\n",
        "\n",
        "fig, ax = plt.subplots(nrows=2, ncols=3, figsize=[10,5])\n",
        "\n",
        "for count,dataset_name in enumerate(dataset_names):\n",
        "    dataset = get_dataset(dataset_name)\n",
        "    dataset_diff = tdiff.apply(dataset)\n",
        "\n",
        "    ax[0][count].plot(dataset)\n",
        "    ax[1][count].plot(dataset_diff)\n",
        "    ax[0][count].set_title(dataset_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFqFjJBi3gMr"
      },
      "source": [
        "##Auxiliary Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVX0Aa9USDpw"
      },
      "source": [
        "def synthetic_dataframe(file, tag, measure, transformation, benchmark_model):\n",
        "    \n",
        "    measure_time = 'time'\n",
        "    \n",
        "    df_time = bUtil.get_dataframe_from_bd(file,\n",
        "                                     \"tag = '\"+tag+\"' and measure = '\"+measure_time+\"' \"+\n",
        "                                     \"and transformation is \" + (\" not null \" if transformation else \" null \") + \n",
        "                                     \"and partitions is \" + (\"null \" if benchmark_model else \"not null\" ))\n",
        "    '--------------------------------'\n",
        "\n",
        "\n",
        "    df = bUtil.get_dataframe_from_bd(file,\n",
        "                                     \"tag = '\"+tag+\"' and measure = '\"+measure+\"' \"+\n",
        "                                     \"and transformation is \" + (\" not null \" if transformation else \" null \") + \n",
        "                                     \"and partitions is \" + (\"null \" if benchmark_model else \"not null\" ))\n",
        "    data = []\n",
        "\n",
        "    models = df.Model.unique()\n",
        "    datasets = df.Dataset.unique()\n",
        "    for dataset in datasets:\n",
        "        for model in models:\n",
        "            _filter = (df.Dataset == dataset) & (df.Model == model)\n",
        "            orders = df[_filter].Order.unique()\n",
        "            partitions = df[_filter].Partitions.unique()\n",
        "            for order in orders:\n",
        "                if benchmark_model:\n",
        "                    _filter2 = (df.Dataset == dataset) & (df.Model == model)  & (df.Order == order) \n",
        "                    avg = np.nanmean(df[_filter2].Value)\n",
        "                    std = np.nanstd(df[_filter2].Value)\n",
        "                    rules = \"No\"\n",
        "                    tempo = np.sum(df_time[_filter2].Value)\n",
        "                    data.append([dataset, model, transformation, order, None, avg, std,rules,tempo])\n",
        "                else:\n",
        "                    for partition in partitions:\n",
        "                        _filter2 = (df.Dataset == dataset) & (df.Model == model)  & (df.Order == order) & (df.Partitions == partition)\n",
        "                        avg = np.nanmean(df[_filter2].Value)\n",
        "                        std = np.nanstd(df[_filter2].Value)\n",
        "                        rules = np.nanmean(df[_filter2].Size)\n",
        "                        tempo = np.sum(df_time[_filter2].Value)\n",
        "                        data.append([dataset, model, transformation, order, partition, avg, std,rules,tempo])\n",
        "\n",
        "    dat = pd.DataFrame(data,columns=['Dataset','Model','Transformation','Order','Partitions','AVG','STD','Rules','Tempo'])\n",
        "    dat = dat.sort_values(['AVG','STD'])\n",
        " \n",
        "    best = []\n",
        "\n",
        "    for dataset in datasets:\n",
        "        for model in models:\n",
        "            orders = dat[(dat.Dataset == dataset) & (dat.Model == model)].Order.unique()\n",
        "            for order in orders:\n",
        "                ix = dat[(dat.Dataset == dataset) &  (dat.Model == model) & (dat.Order == order)].index[0]\n",
        "                best.append(ix)\n",
        "                \n",
        "    ret = dat.loc[best].sort_values(['AVG','STD'])\n",
        "    ret.groupby('Dataset')\n",
        "\n",
        "    return ret\n",
        "    \n",
        "def filter_db_by(file, df, tag, measure):\n",
        "    sql = \"tag = '\" + tag + \"' and measure = '\" + measure +\"' and (\"\n",
        "    tmpsql = \"\"\n",
        "    df.index = np.arange(len(df.index))\n",
        "    for ix in df.index:\n",
        "        row = df.loc[ix]\n",
        "        try:\n",
        "            tmp = row.pop(\"AVG\")\n",
        "            tmp = row.pop(\"STD\")\n",
        "            part = row.pop('Partitions')\n",
        "            transf = row.pop('Transformation')\n",
        "            order = row.pop('Order')\n",
        "            sql2 = \"\"\n",
        "            for key in row.keys():\n",
        "                if len(sql2) > 0:\n",
        "                    sql2 += \" and \"\n",
        "                sql2 += key +\"='\"+str(row[key])+\"'\"\n",
        "            sql2 += \" and benchmarks.'order' = \" + str(order)\n",
        "            sql2 += \" and Partitions \" + (\"= \" + str(part) if part is not None else \" is null \")     \n",
        "            sql2 += \" and Transformation \" + (\" is not null \" if transf  else \" is null \")\n",
        "\n",
        "            if len(tmpsql) > 0:\n",
        "                tmpsql += \" or \"\n",
        "\n",
        "            tmpsql += \"(\" + sql2 + \")\"\n",
        "        except Exception as ex:\n",
        "            print(ex)\n",
        "            print(row)\n",
        "        \n",
        "    sql += tmpsql + \")\"\n",
        "        \n",
        "    return bUtil.get_dataframe_from_bd(file,sql)\n",
        "            \n",
        "    \n",
        "def split_measurements_by(df, dataset, exclude=[], dump=False):\n",
        "    ret = []\n",
        "    mods = []\n",
        "    models = df.Model.unique()\n",
        "    for model in models:\n",
        "        test = np.any([model.rfind(k) != -1 for k in exclude]) if len(exclude) > 0 else False\n",
        "        if not test:\n",
        "            orders = df[(df.Dataset == dataset) & (df.Model == model)].Order.unique()\n",
        "            if len(orders) > 0:\n",
        "                for order in orders:\n",
        "                    if dump: print(model,order)\n",
        "                    values = df[(df.Dataset == dataset) & (df.Model == model) & (final.Order == order)][\"Value\"].values \n",
        "                    ret.append(values)\n",
        "                    if dump: print(len(values))\n",
        "            else:\n",
        "                if dump: print(model)\n",
        "                values = df[(df.Dataset == dataset) & (df.Model == model)][\"Value\"].values\n",
        "                ret.append(values)\n",
        "                if dump: print(len(values))\n",
        "            mods.append(model + str(order))\n",
        "\n",
        "        \n",
        "    return (ret, mods)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8RbumB8zJ7d"
      },
      "source": [
        "##Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6as-Eh63QDR"
      },
      "source": [
        "**CAUTION:** This task is computationally expensive and takes several hours to be performed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCo7R0JqyuV2"
      },
      "source": [
        "from pyFTS.common import Transformations\n",
        "from pyFTS.data import TAIEX, SP500, NASDAQ\n",
        "from pyFTS.benchmarks import benchmarks as bchmk, Util as bUtil\n",
        "from pyFTS.benchmarks import Measures, naive, arima, ResidualAnalysis, quantreg, knn\n",
        "\n",
        "tdiff = Transformations.Differential(1)\n",
        "types = ['point']\n",
        "dataset_names = [\"SP500\", \"TAIEX\",\"NASDAQ\"]\n",
        "\n",
        "tag = \"fre\"\n",
        "\n",
        "benchmark_methods=[\n",
        "    [arima.ARIMA for k in range(4)] + [naive.Naive],\n",
        "    [arima.ARIMA for k in range(8)] + [quantreg.QuantileRegression for k in range(4)],\n",
        "    [arima.ARIMA for k in range(4)] + [quantreg.QuantileRegression for k in range(2)] + [knn.KNearestNeighbors for k in range(3)]\n",
        "    ]\n",
        "\n",
        "benchmark_methods_parameters= [\n",
        "    [\n",
        "        {'order': (1, 0, 0)},\n",
        "        {'order': (1, 0, 1)},\n",
        "        {'order': (2, 0, 1)},\n",
        "        {'order': (2, 0, 2)},\n",
        "        {},\n",
        "    ],[\n",
        "        {'order': (1, 0, 0), 'alpha': .05},\n",
        "        {'order': (1, 0, 0), 'alpha': .25},\n",
        "        {'order': (1, 0, 1), 'alpha': .05},\n",
        "        {'order': (1, 0, 1), 'alpha': .25},\n",
        "        {'order': (2, 0, 1), 'alpha': .05},\n",
        "        {'order': (2, 0, 1), 'alpha': .25},\n",
        "        {'order': (2, 0, 2), 'alpha': .05},\n",
        "        {'order': (2, 0, 2), 'alpha': .25},\n",
        "        {'order': 1, 'alpha': .05},\n",
        "        {'order': 1, 'alpha': .25},\n",
        "        {'order': 2, 'alpha': .05},\n",
        "        {'order': 2, 'alpha': .25}\n",
        "    ],[\n",
        "        {'order': (1, 0, 0)},\n",
        "        {'order': (1, 0, 1)},\n",
        "        {'order': (2, 0, 1)},\n",
        "        {'order': (2, 0, 2)},\n",
        "        {'order': 1, 'dist': True},\n",
        "        {'order': 2, 'dist': True},\n",
        "        {'order': 1}, {'order': 2}, {'order': 3},\n",
        "    ]\n",
        "]\n",
        "\n",
        "for dataset_name in dataset_names:\n",
        "\n",
        "  dataset = get_dataset(dataset_name)\n",
        "\n",
        "  for ct, _type in enumerate(types):\n",
        "      \n",
        "      bchmk.sliding_window_benchmarks(dataset, 1000, train=0.8, inc=0.2,\n",
        "                                      benchmark_models=True,\n",
        "                                      benchmark_methods=benchmark_methods[ct],\n",
        "                                      benchmark_methods_parameters=benchmark_methods_parameters[ct],\n",
        "                                      transformations=[tdiff],\n",
        "                                      orders=[1,2,3],\n",
        "                                      partitions=np.arange(1, 51), \n",
        "                                      progress=False, type=_type,\n",
        "                                      steps_ahead=[1],\n",
        "                                      #distributed=True, nodes=['192.168.0.110', '192.168.0.107', '192.168.0.106'],\n",
        "                                      file=\"benchmarks.db\", dataset=dataset_name, tag=tag)\n",
        " \n",
        "\n",
        "'Generate the dataframes from the sql database'\n",
        "abc1 = synthetic_dataframe(\"benchmarks.db\",\"fre\",\"rmse\",True,False)\n",
        "abc2 = synthetic_dataframe(\"benchmarks.db\",\"fre\",\"rmse\",True,True)\n",
        "_filter = pd.concat([abc1,abc2])\n",
        "\n",
        "'Save to excel'\n",
        "nome_arquivo = 'PYFTS_all_models.xlsx'\n",
        "writer = pd.ExcelWriter(nome_arquivo, engine='xlsxwriter')\n",
        "_filter.to_excel(writer, sheet_name='Results')\n",
        "writer.save()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}